<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Locks</title>
        <link rel="stylesheet" href=/assets/css/styles.css>
    </head>
    <body>
        <div class="l-flex-container">
            <div class="l-small-column">
                <nav>
    <ul>
        <li><a href="">Home</a></li>
        
            <li>
                <a href="/topics/01_resource.html">
                    Resource Sharing
                </a>
            </li>
        
            <li>
                <a href="/topics/02_processes.html">
                    More About Processes
                </a>
            </li>
        
            <li>
                <a href="/topics/03_lde.html">
                    Limited Direct Execution
                </a>
            </li>
        
            <li>
                <a href="/topics/04_scheduling.html">
                    Scheduling
                </a>
            </li>
        
            <li>
                <a href="/topics/05_threads.html">
                    Threads and Concurrency
                </a>
            </li>
        
            <li>
                <a href="/topics/06_locks.html">
                    Locks
                </a>
            </li>
        
            <li>
                <a href="/topics/07_condition.html">
                    Condition Variables
                </a>
            </li>
        
            <li>
                <a href="/topics/08_problems.html">
                    Concurrency Problems
                </a>
            </li>
        
    </ul>
</nav>

            </div><!-- .l-small-column -->
            <div class="l-large-column">
                <header>
    <h1>Marcus's Operating System Notes</h1>
</header>

                <main><h1 id="locks">Locks</h1>

<h2 id="locks-and-critical-sections">Locks and Critical Sections</h2>

<p><strong>Locks</strong> are one way to implement critical sections. Metaphorically, a thread locks a door on the way in and then unlocks it on its way out. There are two operations associated with locks:</p>

<ul>
  <li>Acquire: “Wait until the lock is free, then take it to enter a C.S.”</li>
  <li>Release: “Release lock to leave a C.S., waking up anyone waiting for it.”</li>
</ul>

<p>These two functions must <em>always</em> go together. Once you use acquire, you must later use release. If one thread has acquired a lock, another cannot acquire it until the first has released it. When there are multiple threads needing to acquire a lock, acquisition is scheduled on a first-come, first-serve basis. Here is how you would acquire and release in C:</p>

<figure class="highlight"><pre><code class="language-c" data-lang="c"><span class="n">pthread_mutex_t</span> <span class="n">lock</span> <span class="o">=</span> <span class="n">PTHREAD_MUTEX_INITIALIZER</span><span class="p">;</span>
<span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="p">);</span>
<span class="c1">// Critical section code goes here.</span>
<span class="n">pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="p">);</span></code></pre></figure>

<p>The variable <code class="language-plaintext highlighter-rouge">lock</code> is either locked or unlocked. It is initialized to locked.</p>

<p>There are two design approaches for locks: coarse-grained locking, in which “[o]ne big lock is used any time any critical section is accessed,” and fine-grained locking, in which different locks are used “to protect different data and data structures.” This is a design choice that is up to the developer.</p>

<h2 id="lock-implementations">Lock Implementations</h2>

<p>One way to implement locks on the OS is the “spin lock,” where a thread waits to enter a locked critical section by “spinning,” which is when it merely burns cycles in a while loop. Lock implementation on the OS should be <strong>atomic</strong>. “An atomic operation is one which executes as though it could not be interrupted.” Something is required from the hardware to make an operation atomic. An example of an atomic operation is <strong>test-and-set</strong>, where an old value in memory is fetched, the new value is written instead, and the old value is returned. When executing a test-and-set instruction on a lock “flag,” the new value should be 1, i.e., locked. The new value is always 1, too. The returned value could be either 0 or 1. <em>Test-and-set has to be something supported on the hardware itself for all of this to work.</em></p>

<p>Spin locks ensure correctness, but they do not guarantee fairness. Once a lock is in a critical section, there is nothing to force it to give up control. The starvation problem shows up here, too. Performance is clearly a problem because of the “spinning.” Thus, spin locks are too wasteful to be considered an optimal implementation.</p>

<p>So what’s the solution? Yield! Instead of burning cycles in a while loop, the thread should simply yield control of the CPU when it detects a critical section is locked. This will change the thread’s state from “running” to “ready.” This context switch does come with some overhead and starvation still isn’t prevented, so you don’t get better performance for free.</p>

<p>The <strong>fetch-and-add</strong> operation ensures fairness. It is like test-and-set except instead of setting a new value, you add 1 to the old value. We can use this operation to improve our lock implementation. Instead of merely being a flag, the lock has a “ticket” and a “turn.” If the lock’s turn is equal to a thread’s ticket, then the thread is given entry; otherwise, the thread yields. Every time a thread releases a lock, the lock’s “turn” value is fetched-and-added. (We have to assume that a thread cannot be terminated by something outside it in this implementation.)</p>

<h2 id="lock-based-data-structures">Lock-Based Data Structures</h2>

<p>“Adding locks to shared data structures make them thread safe.” This means we do not have to implement a critical section every time access is needed to them.</p>

<h3 id="sloppy-counter">Sloppy Counter</h3>

<p>If you’ve got a shared counter, you can lock the counter when you want to increment it. Synchronized counters have poor performance, however. We say they “scale” poorly. The solution is a <strong>sloppy counter</strong>, which is where each core has its own counter that updates the global counter periodically. The sloppy counters each have a sloppiness threshold. Once the counter reaches that threshold, this value is added to the global counter and the sloppy counter is reset. The rationale behind sloppy counters is the independence of CPU cores.</p>

<h3 id="concurrent-linked-list">Concurrent Linked List</h3>

<p>A <strong>concurrent linked list</strong> has a lock for the head of the linked list. Insertions are protected. Here is the procedure:</p>

<ol>
  <li>Lock.</li>
  <li>Allocate space in the heap for the new node.</li>
  <li>If the allocation failed, throw an error and unlock. Do not forget to unlock!</li>
  <li>Create the new node.</li>
  <li>Update the head as the new node.</li>
  <li>Unlock.</li>
</ol>

<p>If it fails, return -1. If it is successful, return 0. The lookup operation works similarly: each element is locked and then is unlocked as the list is traversed. Alternatively, you can lock and unlock only the head; however, locking an entire list means users have to wait for a thread to finish with a list before performing their own operation on it. The solution is <strong>hand-over-hand locking</strong> in which “[t]raversal involves handing over previous node’s lock, acquiring the next node’s lock, and so on.” This does unfortunately come at a performance cost.</p>

<h3 id="michael--scott-concurrent-queues">Michael &amp; Scott Concurrent Queues</h3>

<p>In a <strong>Michael and Scott concurrent queue</strong>, there are two locks: one for the head and the other for the tail. This allows items to “be added and removed by separate threads at the same time.” When we have only one item in such a queue, rather than having the head and tail point to the same node, the tail points to a dummy node. The operations enqueue and dequeue are synchronized. Here’s the procedure for enqueue:</p>

<ol>
  <li>Determine if space can be allocated for the new node.</li>
  <li>If so, create a temporary node.</li>
  <li>Lock the tail.</li>
  <li>Add this new node to the queue.</li>
  <li>Unlock the tail.</li>
</ol>

<p>To dequeue:</p>

<ol>
  <li>Lock the head.</li>
  <li>Make the temporary node the head.</li>
  <li>The new head is the one pointed to by the temporary node.</li>
  <li>If the new head is not <code class="language-plaintext highlighter-rouge">NULL</code>, unlock the head and free the temporary node.</li>
</ol>
</main>
                <footer>
    <p>
    The information on this page should be distributed freely and must be used
    for educational purposes only. While this website is wholly the creation of
    Marcus Bartlett, all content is from Professor Juhau Hu's TCSS 422
    class at University of Washington Tacoma during the quarter of Spring 2022.
    Some course content is therefore adapted from the course textbook
    <a href="https://pages.cs.wisc.edu/~remzi/OSTEP/">Operating Systems: Three
        Easy Pieces
    </a> by Remzi and Andrea Arpaci-Dusseau
    The header image was created by
    <a href="https://unsplash.com/photos/jXd2FSvcRr8">Umberto</a>. Anything that
    appears in quotation marks without citations is taken from Juhua's
    lecture slides. Please do not send the copyright police after me.
    </p>
    <p>
        If you see any problem with this website, please email me at
        <a href="mailto:mabartlett@hotmail.com">mabartlett@hotmail.com</a>.
    </p>
</footer>

            </div><!-- l-large-column -->
        </div><!-- .l-flex-container -->
    </body>
</html>
